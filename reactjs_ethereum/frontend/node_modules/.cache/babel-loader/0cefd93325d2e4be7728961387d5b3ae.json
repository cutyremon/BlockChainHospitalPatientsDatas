{"ast":null,"code":"'use strict';\n\nvar importer = require('ipfs-unixfs-importer');\n\nvar kindOf = require('kind-of');\n\nvar toAsyncIterator = require('pull-stream-to-async-iterator');\n\nvar toPullStream = require('async-iterator-to-pull-stream');\n\nvar pull = require('pull-stream/pull');\n\nvar pullValues = require('pull-stream/sources/values');\n\nvar pullMap = require('pull-stream/throughs/map');\n\nvar pullAsyncMap = require('pull-stream/throughs/async-map');\n\nvar pullFlatten = require('pull-stream/throughs/flatten');\n\nvar toPull = require('stream-to-pull-stream');\n\nvar waterfall = require('async/waterfall');\n\nvar isStream = require('is-stream');\n\nvar _require = require('is-pull-stream'),\n    isSource = _require.isSource;\n\nvar _require2 = require('./utils'),\n    parseChunkerString = _require2.parseChunkerString;\n\nvar streamFromFileReader = require('ipfs-utils/src/streams/stream-from-filereader');\n\nvar _require3 = require('ipfs-utils/src/supports'),\n    supportsFileReader = _require3.supportsFileReader;\n\nfunction noop() {}\n\nfunction prepareFile(file, self, opts, callback) {\n  opts = opts || {};\n  var cid = file.cid;\n  waterfall([function (cb) {\n    return opts.onlyHash ? cb(null, file) : self.object.get(file.cid, Object.assign({}, opts, {\n      preload: false\n    }), cb);\n  }, function (node, cb) {\n    if (opts.cidVersion === 1) {\n      cid = cid.toV1();\n    }\n\n    var b58Hash = cid.toBaseEncodedString();\n    var size = node.size;\n\n    if (Buffer.isBuffer(node)) {\n      size = node.length;\n    }\n\n    cb(null, {\n      path: file.path === undefined ? b58Hash : file.path || '',\n      hash: b58Hash,\n      // multihash: b58Hash,\n      size: size\n    });\n  }], callback);\n}\n\nfunction normalizeContent(content, opts) {\n  if (!Array.isArray(content)) {\n    content = [content];\n  }\n\n  return content.map(function (data) {\n    if (supportsFileReader && kindOf(data) === 'file') {\n      data = {\n        path: '',\n        content: toPull.source(streamFromFileReader(data))\n      };\n    } // Buffer input\n\n\n    if (Buffer.isBuffer(data)) {\n      data = {\n        path: '',\n        content: pullValues([data])\n      };\n    } // Readable stream input\n\n\n    if (isStream.readable(data)) {\n      data = {\n        path: '',\n        content: toPull.source(data)\n      };\n    }\n\n    if (isSource(data)) {\n      data = {\n        path: '',\n        content: data\n      };\n    }\n\n    if (data && data.content && typeof data.content !== 'function') {\n      if (supportsFileReader && kindOf(data.content) === 'file') {\n        data = {\n          path: data.path,\n          content: toPull.source(streamFromFileReader(data.content))\n        };\n      }\n\n      if (Buffer.isBuffer(data.content)) {\n        data = {\n          path: data.path,\n          content: pullValues([data.content])\n        };\n      }\n\n      if (isStream.readable(data.content)) {\n        data = {\n          path: data.path,\n          content: toPull.source(data.content)\n        };\n      }\n    }\n\n    if (opts.wrapWithDirectory && !data.path) {\n      throw new Error('Must provide a path when wrapping with a directory');\n    }\n\n    return data;\n  });\n}\n\nfunction preloadFile(file, self, opts) {\n  var isRootFile = !file.path || opts.wrapWithDirectory ? file.path === '' : !file.path.includes('/');\n  var shouldPreload = isRootFile && !opts.onlyHash && opts.preload !== false;\n\n  if (shouldPreload) {\n    self._preload(file.hash);\n  }\n\n  return file;\n}\n\nfunction pinFile(file, self, opts, cb) {\n  // Pin a file if it is the root dir of a recursive add or the single file\n  // of a direct add.\n  var pin = 'pin' in opts ? opts.pin : true;\n  var isRootDir = !file.path.includes('/');\n  var shouldPin = pin && isRootDir && !opts.onlyHash && !opts.hashAlg;\n\n  if (shouldPin) {\n    return self.pin.add(file.hash, {\n      preload: false\n    }, function (err) {\n      return cb(err, file);\n    });\n  } else {\n    cb(null, file);\n  }\n}\n\nmodule.exports = function (self) {\n  // Internal add func that gets used by all add funcs\n  return function addPullStream(options) {\n    options = options || {};\n    var chunkerOptions;\n\n    try {\n      chunkerOptions = parseChunkerString(options.chunker);\n    } catch (err) {\n      return pullMap(function () {\n        throw err;\n      });\n    }\n\n    var opts = Object.assign({}, {\n      shardSplitThreshold: self._options.EXPERIMENTAL.sharding ? 1000 : Infinity\n    }, options, {\n      chunker: chunkerOptions.chunker,\n      chunkerOptions: chunkerOptions.chunkerOptions\n    }); // CID v0 is for multihashes encoded with sha2-256\n\n    if (opts.hashAlg && opts.cidVersion !== 1) {\n      opts.cidVersion = 1;\n    }\n\n    var total = 0;\n    var prog = opts.progress || noop;\n\n    var progress = function progress(bytes) {\n      total += bytes;\n      prog(total);\n    };\n\n    opts.progress = progress;\n    return pull(pullMap(function (content) {\n      return normalizeContent(content, opts);\n    }), pullFlatten(), pullMap(function (file) {\n      return {\n        path: file.path ? file.path : undefined,\n        content: file.content ? toAsyncIterator(file.content) : undefined\n      };\n    }), toPullStream.transform(function (source) {\n      return importer(source, self._ipld, opts);\n    }), pullAsyncMap(function (file, cb) {\n      return prepareFile(file, self, opts, cb);\n    }), pullMap(function (file) {\n      return preloadFile(file, self, opts);\n    }), pullAsyncMap(function (file, cb) {\n      return pinFile(file, self, opts, cb);\n    }));\n  };\n};","map":null,"metadata":{},"sourceType":"script"}